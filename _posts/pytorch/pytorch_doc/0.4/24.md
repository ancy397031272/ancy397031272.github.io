# torch.utils.bottleneck

`torch.utils.bottleneck`是一个可以用作调试程序瓶颈的初始步骤的工具。它汇总了`Python`分析器和`PyTorch`的`autograd`分析器脚本的运行情况。

用命令行运行它

```py
python -m torch.utils.bottleneck /path/to/source/script.py [args]
```

`[args]`是`script.py`中的任意参数，也可以运行如下代码获取更多使用说明。

```py
python -m torch.utils.bottleneck -h
```

> 警告 由于您的脚本将被分析，请确保它在有限的时间内退出。
> 
> 警告 由于CUDA内核的异步特性，在针对CUDA代码运行时，cProfile输出和CPU模式autograd分析器可能无法显示以正确的顺序显示：报告的CPU时间报告启动内核所用的时间量，但不包括时间内核花在GPU上执行，除非操作进行同步。在常规CPU模式分析器下，进行同步的Ops似乎非常昂贵。在这些情况下，时序不正确，CUDA模式autograd分析器可能会有所帮助。

* * *

> 注意 要决定要查看哪个(纯CPU模式或CUDA模式）autograd分析器输出，应首先检查脚本是否受CPU限制(“CPU总时间远远超过CUDA总时间”）。如果它受到CPU限制，查看CPU模式autograd分析器的结果将有所帮助。另一方面，如果脚本大部分时间都在GPU上执行，那么开始在CUDA模式autograd分析器的输出中查找负责任的CUDA操作符是有意义的。 当然，实际情况要复杂得多，根据你正在评估的模型部分，你的脚本可能不会处于这两个极端之一。如果探查器输出没有帮助，你可以尝试寻找的结果torch.autograd.profiler.emit_nvtx()用nvprof。但是，请注意NVTX的开销非常高，并且经常会产生严重偏斜的时间表。

* * *

> 警告 如果您正在分析CUDA代码，那么bottleneck运行的第一个分析器(cProfile）将在其时间报告中包含CUDA启动时间(CUDA缓冲区分配成本）。如果您的瓶颈导致代码比CUDA启动时间慢得多，这应该没有关系。

有关分析器的更复杂用途(如多`GPU`情况下），请参阅https://docs.python.org/3/library/profile.html 或使用`torch.autograd.profiler.profile()`获取更多信息。

### 译者署名

| 用户名 | 头像 | 职能 | 签名 |
| --- | --- | --- | --- |
| [Song](https://ptorch.com) | ![](img/2018033000352689884.jpeg) | 翻译 | 人生总要追求点什么 |

