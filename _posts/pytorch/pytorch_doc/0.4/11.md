

# torch.Tensor

`torch.Tensor`是一种包含单一数据类型元素的多维矩阵。

Torch定义了七种CPU张量类型和八种GPU张量类型：

| Data tyoe | CPU tensor | GPU tensor |
| --- | --- | --- |
| 32-bit floating point | `torch.FloatTensor` | `torch.cuda.FloatTensor` |
| 64-bit floating point | `torch.DoubleTensor` | `torch.cuda.DoubleTensor` |
| 16-bit floating point | N/A | `torch.cuda.HalfTensor` |
| 8-bit integer (unsigned) | `torch.ByteTensor` | `torch.cuda.ByteTensor` |
| 8-bit integer (signed) | `torch.CharTensor` | `torch.cuda.CharTensor` |
| 16-bit integer (signed) | `torch.ShortTensor` | `torch.cuda.ShortTensor` |
| 32-bit integer (signed) | `torch.IntTensor` | `torch.cuda.IntTensor` |
| 64-bit integer (signed) | `torch.LongTensor` | `torch.cuda.LongTensor` |

`torch.Tensor`是默认的tensor类型(`torch.FlaotTensor`）的简称。

张量可以从Python的`list`或序列构成​​：

```py
>>> torch.FloatTensor([[1, 2, 3], [4, 5, 6]])
1 2 3
4 5 6
[torch.FloatTensor of size 2x3]
```

可以通过指定它的大小来构建一个空的张量：

```py
>>> torch.IntTensor(2, 4).zero_()
0 0 0 0
0 0 0 0
[torch.IntTensor of size 2x4]
```

可以使用Python的索引和切片符号来访问和修改张量的内容：

```py
>>> x = torch.FloatTensor([[1, 2, 3], [4, 5, 6]])
>>> print(x[1][2])
6.0
>>> x[0][1] = 8
>>> print(x)
 1 8 3
 4 5 6
[torch.FloatTensor of size 2x3]
```

每一个张量tensor都有一个相应的`torch.Storage`保存其数据。张量类提供了一个多维的、横向视图的存储，并定义了数字操作。

> 注意： 改变张量的方法可以用一个下划线后缀来标示。比如，`torch.FloatTensor.abs_()`会在原地计算绝对值并返回修改的张量，而`tensor.FloatTensor.abs()`将会在新张量中计算结果。

```py
class torch.Tensor
class torch.Tensor(*sizes)
class torch.Tensor(size)
class torch.Tensor(sequence)
class torch.Tensor(ndarray)
class torch.Tensor(tensor)
class torch.Tensor(storage)
```

从可选的大小或数据创建新的张量。 如果没有给出参数，则返回空的零维张量。如果提供了`numpy.ndarray`,`torch.Tensor`或`torch.Storage`，将会返回一个相同数据的新张量.如果提供了python序列，则从序列的副本创建一个新的张量。

#### abs()

参考`torch.abs()`,矩阵数组绝对值

#### abs_()

`abs()`的直接运算形式

#### acos()

参考`torch.acos()`

#### acos_()

`acos()`的直接运算形式，即直接执行并且返回修改后的张量

#### add(value)

参考`torch.add()`

#### add_(value)

`add()`的直接运算形式，即直接执行并且返回修改后的张量

#### addbmm(beta=1, mat, alpha=1, batch1, batch2)

参考`torch.addbmm()`

#### addbmm_(beta=1, mat, alpha=1, batch1, batch2)

`addbmm()`的直接运算形式，即直接执行并且返回修改后的张量

#### addcdiv(value=1, tensor1, tensor2)

参考`torch.addcdiv()`

#### addcdiv_(value=1, tensor1, tensor2)

`addcdiv()`的直接运算形式，即直接执行并且返回修改后的张量

#### addcmul(value=1, tensor1, tensor2)

参考`torch.addcmul()`

#### addcmul_(value=1, tensor1, tensor2)

`addcmul()`的直接运算形式，即直接执行并且返回修改后的张量

#### addmm(beta=1, mat, alpha=1, mat1, mat2)

参考`torch.addmm()`

#### addmm_(beta=1, mat, alpha=1, mat1, mat2)

`addmm()`的直接运算形式，即直接执行并且返回修改后的张量

#### addmv(beta=1, tensor, alpha=1, mat, vec)

参考`torch.addmv()`

#### addmv_(beta=1, tensor, alpha=1, mat, vec)

`addmv()`的直接运算形式，即直接执行并且返回修改后的张量

#### addr(beta=1, alpha=1, vec1, vec2)

参考`torch.addr()`

#### addr_(beta=1, alpha=1, vec1, vec2)

`addr()`的直接运算形式，即直接执行并且返回修改后的张量

#### apply_(callable)

将函数`callable`作用于tensor中每一个元素，并替换每个元素用`callable`函数返回的值。

> 注意： 该函数只能在CPU tensor中使用，并且不应该用在有较高性能要求的代码块。

#### asin()

参考`torch.asin()`

#### asin_()

`asin()`的直接运算形式，即直接执行并且返回修改后的张量

#### atan()

参考`torch.atan()`

#### atan2()

参考`torch.atan2()`

#### atan2_()

`atan2()`的直接运算形式，即直接执行并且返回修改后的张量

#### atan_()

`atan()`的直接运算形式，即直接执行并且返回修改后的张量

#### baddbmm(beta=1, alpha=1, batch1, batch2)

参考`torch.baddbmm()`

#### baddbmm_(beta=1, alpha=1, batch1, batch2)

`baddbmm()`的直接运算形式，即直接执行并且返回修改后的张量

#### bernoulli()

参考`torch.bernoulli()`

#### bernoulli_()

`bernoulli()`的直接运算形式，即直接执行并且返回修改后的张量

#### bmm(batch2)

参考`torch.bmm()`

#### byte()

将tensor改为byte类型

#### bmm(median=0, sigma=1, *, generator=None)

将tensor中元素用柯西分布得到的数值填充： P(x)=1/π * σ/(x−median)2+σ2

#### ceil()

参考`torch.ceil()`

#### ceil_()

`ceil()`的直接运算形式，即直接执行并且返回修改后的张量

#### char()

将tensor元素改为char类型

#### chunk(n_chunks, dim=0)

将tensor分割为tensor元组. 参考`torch.chunk()`

#### clamp(min, max)

参考`torch.clamp()`

#### clamp_(min, max)

`clamp()`的直接运算形式，即直接执行并且返回修改后的张量

#### clone()

返回与原tensor有相同大小和数据类型的tensor

#### contiguous()

返回一个内存连续的有相同数据的tensor，如果原tensor内存连续则返回原tensor

#### copy_(src, async=False)

将`src`中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。

参数：

*   src(Tensor） - 要复制的源张量
*   async(bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。
*   broadcast(bool） - 如果为True，src将广播到底层张量的形状。

#### cos()

参考`torch.cos()`

#### cos_()

`cos()`的直接运算形式，即直接执行并且返回修改后的张量

#### cosh()

参考`torch.cosh()`

#### cosh_()

`cosh()`的直接运算形式，即直接执行并且返回修改后的张量

#### cpu()

如果在CPU上没有该tensor，则会返回一个CPU的副本

#### cross(other, dim=-1)

参考`torch.cross()`

#### cuda(device=None, async=False)

返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。

参数：

*   device(int） ：目标GPU ID。默认为当前设备。
*   async(bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。

#### cumprod(dim)

参考`torch.cumprod()`

#### cumsum(dim)

参考`torch.cumsum()`

#### data_ptr() → int

返回tensor第一个元素的地址

#### diag(diagonal=0)

参考`torch.diag()`

#### dim() → int

返回tensor的维数

#### dist(other, p=2)

参考`torch.dist()`

#### div(value)

参考`torch.div()`

#### div_(value)

`div()`的直接运算形式，即直接执行并且返回修改后的张量

#### dot(tensor2) → float

参考`torch.dot()`

#### double()

将该tensor投射为double类型

#### eig(eigenvectors=False) -&gt; (Tensor, Tensor)

参考`torch.eig()`

#### element_size() → int

返回单个元素的字节大小。 例：

```py
>>> torch.FloatTensor().element_size()
4
>>> torch.ByteTensor().element_size()
1
```

#### eq(other)

参考`torch.eq()`

#### eq_(other)

`eq()`的直接运算形式，即直接执行并且返回修改后的张量

#### equal(other) → bool

参考`torch.equal()`

#### exp()

参考`torch.exp()`

#### exp_()

`exp()`的直接运算形式，即直接执行并且返回修改后的张量

#### expand(*sizes)

返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将`stride`设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。

参数：

*   sizes(torch.Size or int...)-需要扩展的大小

例：

```py
>>> x = torch.Tensor([[1], [2], [3]])
>>> x.size()
torch.Size([3, 1])
>>> x.expand(3, 4)
 1 1
 1 1
 2 2 2 2
 3 3 3 3
 [torch.FloatTensor of size 3x4]
```

#### expand_as(tensor)

将tensor扩展为参数tensor的大小。 该操作等效与：

```py
self.expand(tensor.size())
```

#### exponential_(lambd=1, *, generator=None) $to$ Tensor

将该tensor用指数分布得到的元素填充： $$ P(x)= \lambda e^{- \lambda x} $$

#### fill_(value)

将该tensor用指定的数值填充

#### float()

将tensor投射为float类型

#### floor()

参考`torch.floor()`

#### floor_()

`floor()`的直接运算形式，即直接执行并且返回修改后的张量

#### fmod(divisor)

参考`torch.fmod()`

#### fmod_(divisor)

`fmod()`的直接运算形式，即直接执行并且返回修改后的张量

#### frac()

参考`torch.frac()`

#### frac_()

`frac()`的直接运算形式，即直接执行并且返回修改后的张量

#### gather(dim, index)

参考`torch.gather()`

#### ge(other)

参考`torch.ge()`

#### ge_(other)

`ge()`的直接运算形式，即直接执行并且返回修改后的张量

#### gels(A)

参考`torch.gels()`

#### geometric_(p, *, generator=None)

将该tensor用几何分布得到的元素填充： $$ P(X=k)= (1-p)^{k-1}p $$

#### geqrf() -&gt; (Tensor, Tensor)

参考`torch.geqrf()`

#### ger(vec2)

参考`torch.ger()`

#### gesv(A), Tensor

参考`torch.gesv()`

#### gt(other)

参考`torch.gt()`

#### gt_(other)

`gt()`的直接运算形式，即直接执行并且返回修改后的张量

#### half()

将tensor投射为半精度浮点类型

#### histc(bins=100, min=0, max=0)

参考`torch.histc()`

#### index(m)

用一个二进制的掩码或沿着一个给定的维度从tensor中选取元素。`tensor.index(m)`与`tensor[m]`完全相同。

参数：

*   m(int or Byte Tensor or slice)-用来选取元素的维度或掩码

    #### index_add_(dim, index, tensor)

    按参数index中的索引数确定的顺序，将参数tensor中的元素加到原来的tensor中。参数tensor的尺寸必须严格地与原tensor匹配，否则会发生错误。

参数：

*   dim(int)-索引index所指向的维度
*   index(LongTensor)-需要从tensor中选取的指数
*   tensor(Tensor)-含有相加元素的tensor

例：

```py
>>> x = torch.Tensor([[1, 1, 1], [1, 1, 1], [1, 1, 1]])
>>> t = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
>>> index = torch.LongTensor([0, 2, 1])
>>> x.index_add_(0, index, t)
>>> x
  2 3 4
  8 9 10
  5 6 7
[torch.FloatTensor of size 3x3]
```

#### index_copy_(dim, index, tensor)

按参数index中的索引数确定的顺序，将参数tensor中的元素复制到原来的tensor中。参数tensor的尺寸必须严格地与原tensor匹配，否则会发生错误。

参数：

*   dim (int)-索引index所指向的维度
*   index (LongTensor)-需要从tensor中选取的指数
*   tensor (Tensor)-含有被复制元素的tensor

例：

```py
>>> x = torch.Tensor(3， 3)
>>> t = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
>>> index = torch.LongTensor([0, 2, 1])
>>> x.index_copy_(0, index, t)
>>> x
  1 2 3
  7 8 9
  4 5 6
[torch.FloatTensor of size 3x3]
```

#### index_fill_(dim, index, val)

按参数index中的索引数确定的顺序，将原tensor用参数`val`值填充。

参数：

*   dim (int)-索引index所指向的维度
*   index (LongTensor)-索引
*   val (Tensor)-填充的值

例：

```py
>>> x = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
>>> index = torch.LongTensor([0, 2])
>>> x.index_fill_(0, index, -1)
>>> x
  -1 2 -1
  -1 5 -1
  -1 8 -1
[torch.FloatTensor of size 3x3]
```

#### index_select(dim, index)

参考`torch.index_select()`

#### int()

将该tensor投射为int类型

#### inverse()

参考`torch.inverse()`

#### is_contiguous() → bool

如果该tensor在内存中是连续的则返回True。

#### is_cuda

#### is_pinned()

如果该tensor在固定内内存中则返回True

#### is_set_to(tensor) → bool

如果此对象引用与Torch C API相同的`THTensor`对象作为给定的张量，则返回True。

#### is_signed()

#### kthvalue(k, dim=None) -&gt; (Tensor, LongTensor)

参考`torch.kthvalue()`

#### le(other)

参考`torch.le()`

#### le_(other)

`le()`的直接运算形式，即直接执行并且返回修改后的张量

#### lerp(start, end, weight)

参考`torch.lerp()`

#### lerp_(_start, end, weight_)

`lerp()`的直接运算形式，即直接执行并且返回修改后的张量

#### log()

参考`torch.log()`

#### loglp()

参考`torch.loglp()`

#### loglp_()

`loglp()`的直接运算形式，即直接执行并且返回修改后的张量

#### log_()→ Tensor

`log()`的直接运算形式，即直接执行并且返回修改后的张量

#### log_normal_(_mwan=1, std=2,_ , gegnerator=None*)

将该tensor用均值为$\mu$,标准差为$\sigma$的对数正态分布得到的元素填充。要注意`mean`和`stdv`是基本正态分布的均值和标准差，不是返回的分布： $$ P(X)= \frac {1} {x \sigma \sqrt {2 \pi}}e^{- \frac {(lnx- \mu)^2} {2 \sigma^2}} $$

#### long()

将tensor投射为long类型

#### lt(_other_)

参考`torch.lt()`

#### lt_(_other_)

`lt()`的直接运算形式，即直接执行并且返回修改后的张量

#### map_(_tensor, callable_)

将`callable`作用于本tensor和参数tensor中的每一个元素，并将结果存放在本tensor中。`callable`应该有下列标志：

```py
def callable(a, b) -> number
```

#### masked_copy_(_mask, source_)

将`mask`中值为1元素对应的`source`中位置的元素复制到本tensor中。`mask`应该有和本tensor相同数目的元素。`source`中元素的个数最少为`mask`中值为1的元素的个数。

参数：

*   mask (_ByteTensor_)-二进制掩码
*   source (_Tensor_)-复制的源tensor

> 注意： `mask`作用于`self`自身的tensor，而不是参数中的`source`。
> 
> #### masked_fill_(_mask, value_)
> 
> 在`mask`值为1的位置处用`value`填充。`mask`的元素个数需和本tensor相同，但尺寸可以不同。形状mask必须 与下面的张量的形状一起广播。

参数：

*   mask (ByteTensor)-二进制掩码
*   value (Tensor)-用来填充的值

#### masked_select(_mask_)

参考`torch.masked_select()`

#### max(_dim=None_) -&gt; _float or(Tensor, Tensor)_

参考`torch.max()`

#### mean(_dim=None_) -&gt; _float or(Tensor, Tensor)_

参考`torch.mean()`

#### median(_dim=-1, value=None, indices=None_) -&gt; _(Tensor, LongTensor)_

参考`torch.median()`

#### min(_dim=None_) -&gt; _float or(Tensor, Tensor)_

参考`torch.min()`

#### mm(_mat2_)

参考`torch.mm()`

#### mode(_dim=-1, value=None, indices=None_) -&gt; _(Tensor, LongTensor)_

参考`torch.mode()`

#### mul(_value_)

参考`torch.mul()`

#### mul_(_value_)

`mul()`的直接运算形式，即直接执行并且返回修改后的张量

#### multinomial(_num_samples, replacement=False,_ , generator=None*)

参考`torch.multinomial()`

#### mv(_vec_)

参考`torch.mv()`

#### narrow(_dimension, start, length_) → Te

返回这个张量的缩小版本的新张量。维度`dim`缩小范围是`start`到`start+length`。返回的张量和该张量共享相同的底层存储。

参数：

*   dimension (_int_)-需要缩小的维度
*   start (_int_)-起始维度
*   length (_int_)-长度

例:

```py
>>> x = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
>>> x.narrow(0, 0, 2)
 1  2  3
 4  5  6
[torch.FloatTensor of size 2x3]
>>> x.narrow(1, 1, 2)
 2  3
 5  6
 8  9
[torch.FloatTensor of size 3x2]
```

#### ndimension() → int

`dim()`的另一种表示。

#### ne(_other_)

参考`torch.ne()`

#### ne_(_other_)

`ne()`的直接运算形式，即直接执行并且返回修改后的张量

#### neg()

参考`torch.neg()`

#### neg_()

`neg()`的直接运算形式，即直接执行并且返回修改后的张量

#### nelement() → int

`numel()`的另一种表示

#### new(*args, **kwargs)

构建一个有相同数据类型的tensor

#### nonezero() → LongTensor

参考`torch.nonezero()

#### norm(_p=2_) → float

参考`torch.norm()

#### normal_(_mean=0, std=1,_ , gengerator=None*)

将tensor用均值为`mean`和标准差为`std`的正态分布填充。

#### numel() → int

参考`numel()`

#### numpy() → ndarray

将该tensor以NumPy的形式返回`ndarray`，两者共享相同的底层内存。原tensor改变后会相应的在`ndarray`有反映，反之亦然。

#### orgqr(_input2_)

参考`torch.orgqr()`

#### ormqr(_input2, input3, left=True, transpose=False_)

参考`torch.ormqr()`

#### permute(_dims_)

将tensor的维度换位。

参数：

*   _dims (_int..*)-换位顺序

例：

```py
>>> x = torch.randn(2, 3, 5)
>>> x.size()
torch.Size([2, 3, 5])
>>> x.permute(2, 0, 1).size()
torch.Size([5, 2, 3])
```

#### pin_memory()

将张量复制到固定内存(如果尚未固定）。

#### potrf(_upper=True_)

参考`torch.potrf()`

#### potri(_upper=True_)

参考`torch.potri()`

#### potrs(_input2, upper=True_)

参考`torch.potrs()`

#### pow(_exponent_)

参考`torch.pow()`

#### pow_()

`pow()`的直接运算形式，即直接执行并且返回修改后的张量

#### prod()) → float

参考`torch.prod()`

#### pstrf(_upper=True, tol=-1_) -&gt; (_Tensor, IntTensor_)

参考`torch.pstrf()`

#### qr()-&gt; (_Tensor, IntTensor_)

参考`torch.qr()`

#### random_(from=0, to=None, *, generator=None)

将tensor用从在[from, to-1]上的正态分布或离散正态分布取样值进行填充。如果未指定，则该值仅由该张量数据类型限定。

#### reciprocal()

参考`torch.reciprocal()`

#### reciprocal_()

`reciprocal()`的直接运算形式，即直接执行并且返回修改后的张量

#### remainder(_divisor_)

参考`torch.remainder()`

#### remainder_(_divisor_)

`remainder()`的直接运算形式，即直接执行并且返回修改后的张量

#### renorm(_p, dim, maxnorm_)

参考`torch.renorm()`

#### renorm_(_p, dim, maxnorm_)

`renorm()`的直接运算形式，即直接执行并且返回修改后的张量

#### repeat(*sizes)

沿指定维度重复此张量。 与`expand()`功能不同，此功能可复制张量中的数据。

参数：

*   *sizes (torch.Size ot int...)-沿着每个维重复这个张量的次数

例：

```py
>>> x = torch.Tensor([1, 2, 3])
>>> x.repeat(4, 2)
 1  2  3  1  2  3
 1  2  3  1  2  3
 1  2  3  1  2  3
 1  2  3  1  2  3
[torch.FloatTensor of size 4x6]
>>> x.repeat(4, 2, 1).size()
torch.Size([4, 2, 3])
```

#### resize_(*sizes)

将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。

参数：

*   sizes (torch.Size or int...)-需要调整的大小

例：

```py
>>> x = torch.Tensor([[1, 2], [3, 4], [5, 6]])
>>> x.resize_(2, 2)
>>> x
 1  2
 3  4
[torch.FloatTensor of size 2x2]
```

#### resize_as_(tensor)

将当前张量调整为与指定张量相同的大小。这相当于：

```py
self.resize_(tensor.size())
```

#### round()

参考`torch.round()`

#### round_()

`round()`的直接运算形式，即直接执行并且返回修改后的张量

#### rsqrt()

参考`torch.rsqrt()`

#### rsqrt_()

`rsqrt()`的直接运算形式，即直接执行并且返回修改后的张量

#### scatter_(dim, index, src)

将`src`中的所有值按照`index`确定的索引写入本tensor中。其中索引是根据给定的dimension，dim按照`gather()`描述的规则来确定。

请注意，对于collect，index的值必须在0和(self.size(dim)-1)之间，包括所有值，并且指定维中的一行中的所有值必须是唯一的。

参数：

*   input (Tensor)-源tensor
*   dim (int)-索引的轴向
*   index (LongTensor)-散射元素的索引指数
*   src (Tensor or float)-散射的源元素

例子：

```py
>>> x = torch.rand(2, 5)
>>> x

 0.4319  0.6500  0.4080  0.8760  0.2355
 0.2609  0.4711  0.8486  0.8573  0.1029
[torch.FloatTensor of size 2x5]

>>> torch.zeros(3, 5).scatter_(0, torch.LongTensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x)

 0.4319  0.4711  0.8486  0.8760  0.2355
 0.0000  0.6500  0.0000  0.8573  0.0000
 0.2609  0.0000  0.4080  0.0000  0.1029
[torch.FloatTensor of size 3x5]

>>> z = torch.zeros(2, 4).scatter_(1, torch.LongTensor([[2], [3]]), 1.23)
>>> z

 0.0000  0.0000  1.2300  0.0000
 0.0000  0.0000  0.0000  1.2300
[torch.FloatTensor of size 2x4]
```

#### select(dim, index) or number

按照index中选定的维度将tensor切片。如果tensor是一维的，则返回一个数字。否则，返回给定维度已经被移除的tensor。

参数：

*   dim (int)-切片的维度
*   index (int)-用来选取的索引

> 注意： `select()`等效于切片。例如：`tensor.select(0, index)`等效于`tensor[index]`，`tensor.select(2, index)`等效于`tensor[:,:,index]`

#### set(source=None, storage_offset=0, size=None, stride=None)

设置底层内存，大小和步长。如果source是张量，则该张量将共享相同的存储空间，并且具有与给定张量相同的大小和步长。另一个则反映在一个张量内的元素变化。

参数：

*   source (Tensor or Storage)-用到的tensor或内存
*   storage_offset (int)-内存的偏移量
*   size (torch.Size)-需要的大小，默认为源tensor的大小。
*   stride(tuple)-需要的步长，默认为C连续的步长。

#### share_memory_()

将底层存储器移动到共享内存。 如果底层存储已经在共享内存和CUDA张量中，不进行任何操作。共享内存中的Tensors无法调整大小。

#### short()

将tensor投射为short类型。

#### sigmoid()

参考`torch.sigmoid()`

#### sigmoid_()

`sidmoid()`的直接运算形式，即直接执行并且返回修改后的张量

#### sign()

参考`torch.sign()`

#### sign_()

`sign()`的直接运算形式，即直接执行并且返回修改后的张量

#### sin()

参考`torch.sin()`

#### sin_()

`sin()`的直接运算形式，即直接执行并且返回修改后的张量

#### sinh()

参考`torch.sinh()`

#### sinh_()

`sinh()`的直接运算形式，即直接执行并且返回修改后的张量

#### size() → torch.Size

返回tensor的大小。返回的值是`tuple`的子类。

例：

```py
>>> torch.Tensor(3, 4, 5).size()
torch.Size([3, 4, 5])
```

#### sort(dim=None, descending=False) -&gt; (Tensor, LongTensor)

参考`torhc.sort()`

#### split(split_size, dim=0)

将tensor分割成tensor数组。 参考`torhc.split()`

#### sqrt()

参考`torch.sqrt()`

#### sqrt_()

`sqrt()`的直接运算形式，即直接执行并且返回修改后的张量

#### squeeze(dim=None)

参考`torch.squeeze()`

#### squeeze_(dim=None)

`squeeze()`的直接运算形式，即直接执行并且返回修改后的张量

#### std() → float

参考`torch.std()`

#### storage() → torch.Storage

返回底层内存。

#### storage_offset() → int

以储存元素的个数的形式返回tensor在地城内存中的偏移量。 例：

```py
>>> x = torch.Tensor([1, 2, 3, 4, 5])
>>> x.storage_offset()
0
>>> x[3:].storage_offset()
3
```

#### _classmethod()_ storage_type()

#### stride()

返回tesnor的步长。

#### sub(value, other)

从该张量中排除一个标量或张量。如果`value`和`other`都是给定的，则在使用之前`other`的每一个元素都会被`value`缩放。 当`other`是一个张量,`other`的形状必须可以与下面的张量的形状一起[广播](https://pytorch.org/docs/master/notes/broadcasting.html#broadcasting-semantics)。

#### sub_(x)

`sub()`的直接运算形式，即直接执行并且返回修改后的张量

#### sum(dim=None)

参考`torch.sum()`

#### svd(some=True) -&gt; (Tensor, Tensor, Tensor)

参考`torch.svd()`

#### symeig(eigenvectors=False, upper=True) -&gt; (Tensor, Tensor)

参考`torch.symeig()`

#### t()

参考`torch.t()`

#### t()

`t()`的直接运算形式，即直接执行并且返回修改后的张量

#### tan()

参考`torch.tan()`

#### tan_()

`tan()`的直接运算形式，即直接执行并且返回修改后的张量

#### tanh()

参考`torch.tanh()`

#### tanh_()

`tanh()`的直接运算形式，即直接执行并且返回修改后的张量

#### tolist()

返回一个tensor的嵌套列表表示。

#### topk(k, dim=None, largest=True, sorted=True) -&gt; (Tensor, LongTensor)

参考`torch.topk()`

#### trace() → float

参考`torch.trace()`

#### transpose(dim0, dim1)

参考`torch.transpose()`

#### transpose(dim0, dim1)

`transpose()`的直接运算形式，即直接执行并且返回修改后的张量

#### tril(k=0)

参考`torch.tril()`

#### tril_(k=0)

`tril()`的直接运算形式，即直接执行并且返回修改后的张量

#### triu(k=0)

参考`torch.triu()`

#### triu(k=0)

`triu()`的直接运算形式，即直接执行并且返回修改后的张量

#### trtrs(A, upper=True, transpose=False, unitriangular=False) -&gt; (Tensor, Tensor)

参考`torch.trtrs()`

#### trunc()

参考`torch.trunc()`

#### trunc()

`trunc()`的直接运算形式，即直接执行并且返回修改后的张量

#### type(new_type=None, async=False)

如果未提供new_type，则返回类型，否则将此对象转换为指定的类型。 如果已经是正确的类型，则不会执行且返回原对象。

参数：

*   new_type (type或string）-需要的类型
*   async (bool)-如果为True，并且源处于固定内存中，目标位于GPU上，反之亦然，则相对于主机异步执行该副本。否则，该参数不发挥作用。

#### type_as(tesnor)

将此张量转换为给定类型的张量。 如果张量已经是正确的类型，则不会执行操作。等效于：

```py
self.type(tensor.type())
```

参数：

*   tensor (Tensor):有所需要类型的tensor

#### unfold(dim, size, step)

返回一个张量，其中包含尺寸size中所有尺寸的维度。 如果_sizedim_是dim维度的原始大小，则返回张量中的维度是(sizedim-size)/step+1

维度大小的附加维度将附加在返回的张量中。

参数：

*   dim (int)- 展开的维度
*   size (int)- 展开的每个切片的大小
*   step (int)-相邻切片之间的步长

例子：

```py
>>> x = torch.arange(1, 8)
>>> x

 1
 2
 3
 4
 5
 6
 7
[torch.FloatTensor of size 7]

>>> x.unfold(0, 2, 1)

 1  2
 2  3
 3  4
 4  5
 5  6
 6  7
[torch.FloatTensor of size 6x2]

>>> x.unfold(0, 2, 2)

 1  2
 3  4
 5  6
[torch.FloatTensor of size 3x2]
```

#### uniform_(from=0, to=1)

用均匀分布采样的数字填充该张量：

#### unsqueeze(dim)

参考`torch.unsqueeze()`

#### unsqueeze_(dim)

`unsqueeze()`的直接运算形式，即直接执行并且返回修改后的张量

#### var()

参考`torch.var()`

#### view(*args)

返回具有相同数据但大小不同的新张量。 返回的张量必须有与原张量相同的数据和相同数量的元素，但可以有不同的大小。一个张量必须是连续`contiguous()`的才能被查看。

例子：

```py
>>> x = torch.randn(4, 4)
>>> x.size()
torch.Size([4, 4])
>>> y = x.view(16)
>>> y.size()
torch.Size([16])
>>> z = x.view(-1, 8)  # the size -1 is inferred from other dimensions
>>> z.size()
torch.Size([2, 8])
```

#### view_as(tensor)

返回被视作与给定的tensor相同大小的原tensor。 等效于：

```py
self.view(tensor.size())
```

#### zero_()

用0填充这个张量。

### 译者署名

| 用户名 | 头像 | 职能 | 签名 |
| --- | --- | --- | --- |
| [Song](https://ptorch.com) | ![](img/2018033000352689884.jpeg) | 翻译 | 人生总要追求点什么 |

