

# 自动差异化包 - torch.autograd

*   [Variable](https://ptorch.com/docs/1/autograd#variable)
*   [API 兼容性](https://ptorch.com/docs/1/autograd#api-compatibility)
*   [变量的直接操作](https://ptorch.com/docs/1/autograd#in-place-operations-on-variables)
*   [In-place 正确性检查](https://ptorch.com/docs/1/autograd#in-place-correctness-checks)
*   [Function](https://ptorch.com/docs/1/autograd#function)

* * *

`torch.autograd`提供了类和函数用来对任意标量函数进行求导。要想使用自动求导，只需要对已有的代码进行微小的改变。只需要将所有的`tensor`包含进`Variable`对象中即可。

### torch.autograd.backward(variables, grad_variables, retain_variables=False)

计算给定变量wrt图叶的梯度的总和。 该图使用链规则进行区分。如果任何variables 非标量(即它们的数据具有多个元素）并且需要梯度，则该函数另外需要指定grad_variables。它应该是一个匹配长度的序列，其包含差分函数wrt对应变量的渐变(None对于不需要梯度张量的所有变量，它是可接受的值）。

此函数在树叶中累加梯度 - 在调用它之前可能需要将其清零。

参数:

*   variables (variable 列表) – 将计算导数的变量 。

*   grad_variables (序列(`Tensor`, `Variable`或者 `None`)) – 渐变写入相应变量的每个元素。任何张量将被自动转换为volatile，除非 create_graph为True。可以为标量变量或不需要grad的值指定无值。如果所有grad_variables都可以接受None值，则该参数是可选的。

*   retain_graph(bool，可选） - 如果为False，则用于计算grad的图形将被释放。请注意，在几乎所有情况下，将此选项设置为True不是必需的，通常可以以更有效的方式解决。默认值为create_graph。

*   create_graph(bool，可选） - 如果为true，则构造导数的图形，允许计算更高阶的衍生产品。默认为False，除非grad_variables包含至少一个非易失性变量。

### torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=None, only_inputs=True)

计算并返回输入的输出梯度的总和。

grad_outputs应该是output 包含每个输出的预先计算的梯度的长度匹配序列。如果输出不需要_ grad，则渐变可以是None）。当不需要派生图的图形时，梯度可以作为Tensors给出，或者作为Variables，在这种情况下将创建图形。

如果only_inputs为True，该函数将仅返回指定输入的渐变列表。如果它是False，则仍然计算所有剩余叶子的渐变度，并将累积到其.grad 属性中。

参数：

*   outputs(可变序列） - 差分函数的输出。
*   inputs(可变序列） - 输入将返回梯度的积分(并不积累.grad）。
*   grad_outputs(Tensor 或Variable的序列） - 渐变wrd每个输出。任何张量将被自动转换为volatile，除非create_graph为True。可以为标量变量或不需要grad的值指定无值。如果所有grad_variables都可以接受None值，则该参数是可选的。
*   retain_graph(bool，可选） - 如果为False，则用于计算grad的图形将被释放。请注意，在几乎所有情况下，将此选项设置为True不是必需的，通常可以以更有效的方式解决。默认值为create_graph。
*   create_graph(bool，可选） - 如果为True，则构造导数的图形，允许计算高阶衍生产品。默认为False，除非grad_variables包含至少一个非易失性变量。
*   only_inputs(bool，可选） - 如果为True，则渐变wrt离开是图形的一部分，但不显示inputs不会被计算和累积。默认为True。

## Variable

### API 兼容性

可变API与常规Tensor API(除了几个就地方法，覆盖渐变计算所需的输入之外）几乎相同。在大多数情况下，传感器可以安全地替换为变量，代码将保持工作正常。因此，我们不会记录变量的所有操作，torch.Tensor为此也应该参考文档。

### 变量的直接操作

支持自动归档中的就地操作是一件很困难的事情，我们在大多数情况下都不鼓励使用它们。Autograd的积极缓冲区释放和重用使其非常高效，并且在现场操作实际上会降低内存使用量的情况下，极少数场合很少。除非您在内存压力很大的情况下运行，否则您可能永远不需要使用它们。

### In-place 正确性检查

所有Variable的跟踪应用于它们的就地操作，并且如果实现检测到在一个功能中保存了向后的变量，但是之后被修改就位，则一旦向后通过开始就会产生错误。这可以确保如果您使用就地功能并且没有看到任何错误，则可以确定计算出的渐变是正确的。

### class torch.autograd.Variable

包裹张量并记录应用的操作。

变量是Tensor对象周围的薄包装，它也保存了渐变wrt，并引用了创建它的函数。此引用允许回溯创建数据的整个操作链。如果变量是由用户创建的，那么它的grad_fn将被None我们称为这样的对象叶变量。

由于自动调整仅支持标量值函数微分，因此grad大小始终与数据大小匹配。此外，grad通常仅分配给叶变量，否则将始终为零。

变量：

*   data – 包含的`Tensor`

*   grad - 变量保持类型和位置的坡度匹配.data。此属性是懒惰分配的，无法重新分配。
*   requires_grad - 指示变量是否由包含任何需要的变量的子图创建的布尔值。有关详细信息，请参阅从向后排除子图。只能在叶变量上更改。
*   volatile - Boolean表示在推断模式下应该使用Variable，即不保存历史记录。有关详细信息，请参阅 从向后排除子图。只能在叶变量上更改。
*   is_leaf - Boolean指示变量是否为图形叶(即如果由用户创建）。
*   grad_fn - 梯度函数图跟踪。

属性:

*   data (any tensor class) – 被包含的`Tensor`

*   requires_grad (bool) – `requires_grad`标记. 只能通过`keyword`传入.

*   volatile (bool) – `volatile`标记. 只能通过`keyword`传入.

#### backward(gradient=None, retain_variables=False)

当前`Variable`对`leaf variable`求偏导。

计算图可以通过链式法则求导。如果`Variable`是 非标量(`non-scalar`)的，且`requires_grad=True`。那么此函数需要指定`gradient`，它的形状应该和`Variable`的长度匹配，里面保存了`Variable`的梯度。

此函数累积`leaf variable`的梯度。你可能需要在调用此函数之前将`Variable`的梯度置零。

参数:

*   grad_variables(Tensor，Variable或None） - 渐变wrt变量。如果它是张量，它将被自动转换为volatile，除非create_graph是True。可以为标量变量或不需要grad的值指定无值。如果无值可接受，则此参数是可选的。
*   retain_graph(bool，可选） - 如果为False，用于计算grads的图形将被释放。请注意，在几乎所有情况下，将此选项设置为True不是必需的，通常可以以更有效的方式解决。默认值为 create_graph。
*   create_graph(bool，可选） - 如果为true，则构造导数的图形，允许计算更高阶的衍生产品。默认为False，除非gradient是volatile变量。

#### detach()

返回一个新变量，与当前图形分离。

结果将永远不需要渐变。如果输入是易失的，输出也将变得不稳定。

> 注意： 返回变量使用与原始变量相同的数据张量，并且可以看到其中任何一个的就地修改，并且可能会触发正确性检查中的错误。

#### detach_()

从创建它的图形中分离变量，使其成为叶。

#### register_hook(hook)

注册一个`backward`钩子。

每次`gradients`被计算的时候，这个`hook`都被调用。`hook`应该拥有以下签名：

```py
hook(grad) -> Variable or None
```

钩子不应该修改它的参数，但是它可以选择返回一个新的渐变，用来代替它grad。

此函数返回一个带有handle.remove() 从模块中删除钩子的方法的句柄。

例:

```py
>>> v = Variable(torch.Tensor([0, 0, 0]), requires_grad=True)
>>> h = v.register_hook(lambda grad: grad * 2)  # double the gradient
>>> v.backward(torch.Tensor([1, 1, 1]))
>>> v.grad.data
 2
 2
 2
[torch.FloatTensor of size 3]
>>> h.remove()  # removes the hook
```

#### reinforce(reward)

注册一个奖励，这个奖励是由一个随机过程得到的。

微分一个随机节点需要提供一个奖励值。如果你的计算图中包含随机 `operations`，你需要在他们的输出上调用这个函数。否则的话，会报错。

参数:

*   reward (Tensor) – 每个元素的reward张量。必须和`Varaible`形状相同，并在同一个设备上。

#### retain_grad()

启用非叶变量的.grad属性。

### class torch.autograd.Function

记录操作历史并定义用于区分操作的公式。

在Variables上执行的每个操作创建一个新的函数对象，执行计算，并记录它发生的情况。历史以DAG的形式保留，边缘表示数据依赖(）。然后，当调用向后时，通过调用每个对象的方法，并将返回的梯度传递给下一个s 来处理图形的拓扑排序 。input &lt;- outputbackward()FunctionFunction

通常，用户与功能交互的唯一方式是创建子类并定义新的操作。这是延长torch.autograd的推荐方法。

由于函数逻辑是大多数脚本的热点，因此几乎所有脚本都被移动到C后端，以确保框架开销最小化。

每个功能仅用于一次(在正向通行证中）。

变量:

*   saved_tensors - 保存在呼叫中的Tensor元组 forward()。
*   saved_variables - 对应于调用中保存的张量的变量元组forward()。
*   needs_input_grad - 长度的布尔元组num_inputs，指示给定输入是否需要渐变。这可以用于优化缓冲区保存为向后，忽略梯度计算backward()。
*   num_inputs - 给出的输入数量forward()。
*   num_outputs - 返回的张量数forward()。
*   requires_grad - 布尔值，表示是否backward()需要调用。

#### backward(* grad_output)

定义用于区分操作的公式。

此功能将被所有子类覆盖。

所有参数都是张量。它必须接受完全一样多的论据，因为许多输出都forward()返回，它应该返回尽可能多的张量，因为有输入forward()。每个参数是给定输出的渐变wrt，每个返回的值应该是对应输入的渐变。

#### static forward(*args, **kwargs)

执行操作。

此功能将被所有子类覆盖。

它可以采取并返回任意数量的张量。

### 译者署名

| 用户名 | 头像 | 职能 | 签名 |
| --- | --- | --- | --- |
| [Song](https://ptorch.com) | ![](img/2018033000352689884.jpeg) | 翻译 | 人生总要追求点什么 |

