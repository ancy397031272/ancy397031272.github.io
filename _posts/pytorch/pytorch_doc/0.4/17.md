

# torch.nn.functional

## 卷积函数

```py
torch.nn.functional.conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor 
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/modules/conv.html#Conv1d)

对由几个平面组成的输入进行卷积操作
对于细节和输出形状，详细可见[Conv1d](https://pytorch.org/docs/master/nn.html#torch.nn.Conv1d)

### 参数：

```py
input：输入的张量形状(minibatch x in_channels x iW)
weight – 过滤器的形状 (out_channels, in_channels, kW) 
bias – 可选偏置的形状(out_channels)默认值：None
stride – 卷积内核的步长，默认为1 
padding – 输入上的隐含零填充。可以是单个数字或元组。默认值:0 
dilation – 内核元素之间的间距。默认值:1 
groups – 将输入分成组，in_channels应该被组数整除。默认值：1　
```

举例：

```py
>>> filters = autograd.Variable(torch.randn(33, 16, 3)
>>> inputs = autograd.Variable(torch.randn(20, 16, 50))
>>> F.conv1d(inputs, filters)
```

* * *

```py
torch.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/modules/conv.html#Conv2d)

在由几个输入平面组成的输入图像上应用2D卷积。
对于细节和输出形状详细可见[Conv2d](https://pytorch.org/docs/master/nn.html#torch.nn.Conv2d)

### 参数：

```py
input – 输入的张量 (minibatch x in_channels x iH x iW) 
weight – 过滤器 (out_channels, in_channels/groups, kH, kW) 
bias – 可选偏置张量(out_channels)。默认值：None 
stride – 卷积核的步长，可以是单个数字或元组(sh x sw）。默认值：1 
padding – 输入中默认0填充。可以是单个数字或元组。默认值:0 
dilation – 核元素之间的间距。默认值：1 
groups – 将输入分成组，in_channels应该被组数整除。默认值：1
```

举例：

```py
>>> # With square kernels and equal stride
>>> filters = torch.randn(8,4,3,3)
>>> inputs = torch.randn(1,4,5,5)
>>> F.conv2d(inputs, filters, padding=1)
```

* * *

```py
torch.nn.functional.conv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/modules/conv.html#Conv3d)

在由几个输入平面组成的输入图像上应用3D卷积。
对于细节和输出形状，查看[Conv3d](https://pytorch.org/docs/master/nn.html#torch.nn.Conv3d)

### 参数：

```py
input – 输入张量的形状 (minibatch x in_channels x iT x iH x iW)
weight – 过滤器的形状 (out_channels x in_channels/groups x kT x kH x kW)
bias – 可选的偏差项的大小(out_channels).默认值是：None
stride – 卷积核的步长，可以是单个数字或元组(st x sh x sw).默认值：1
padding – 在输入中默认的0填充。可以是单个数字或元组(padT, padH, padW).默认值:0
dilation – 核元素之间的间距(dT, dH, dW)。默认值：1
groups – 将输入分成组，in_channels应该被组数整除。默认值：1
```

### 举例：

```py
>>> filters = torch.randn(33, 16, 3, 3, 3)
>>> inputs = torch.randn(20, 16, 50, 10, 20)
>>> F.conv3d(inputs, filters)
```

* * *

```py
torch.nn.functional.conv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) → Tensor
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/modules/conv.html#ConvTranspose1d)

在由几个输入平面组成的输入图像上应用1D转置卷积，有时也被称为去卷积。
有关详细信息和输出形状，参考[ConvTranspose1d](https://pytorch.org/docs/master/nn.html#torch.nn.ConvTranspose1d)。

### 参数：

```py
input：输入的张量形状(minibatch x in_channels x iW)
weight – 过滤器的形状 (in_channels x out_channels/groups x kW) 
bias – 可选偏置的形状(out_channels)默认值：None
stride – 卷积内核的步长，也可以是一个数字或者元组(sW），默认为1 
padding – 输入上的隐含零填充(0≤padding<stride)。可以是单个数字或者元组(padW）。默认值:0 
output_padding-在两端的进行0填充(0≤padding<stride），可以是个单一数字或者元组(out_padW)。默认是0。
dilation – 内核元素之间的间距。可以是单个数字或者元组(dW）。默认值:1 
groups – 将输入分成组，in_channels应该被组数整除。默认值：1
```

### 举例：

```py
>>> inputs = torch.randn(20, 16, 50)
>>> weights = torch.randn(16, 33, 5)
>>> F.conv_transpose1d(inputs, weights)
>
```

* * *

```py
torch.nn.functional.conv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) → Tensor
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/modules/conv.html#ConvTranspose2d)

在由几个输入平面组成的输入图像上应用2D转置卷积，有时也被称为去卷积。
有关详细信息和输出形状，参考[ConvTranspose2d](https://pytorch.org/docs/master/nn.html#torch.nn.ConvTranspose2d)。

### 参数：

```py
input：输入的张量形状(minibatch x in_channels x iH x iW)
weight – 过滤器的形状 (in_channels x out_channels/groups x kH x kW) 
bias – 可选偏置的形状(out_channels)默认值：None
stride – 卷积内核的步长，也可以是一个数字或者元组(sH，sW），默认为1 
padding – 输入上的隐含零填充(0≤padding<stride)。可以是单个数字或者元组(padH，padW）。默认值:0 
output_padding-在两端的进行0填充(0≤padding<stride），可以是个单一数字或者元组( out_padH, out_padW)。默认是0。
dilation – 内核元素之间的间距。可以是单个数字或者元组(dH,dW）。默认值:1 
groups – 将输入分成组，in_channels应该被组数整除。默认值：1
```

### 举例：

```py
>>> # With square kernels and equal stride
>>> inputs = torch.randn(1, 4, 5, 5)
>>> weights = torch.randn(4, 8, 3, 3)
>>> F.conv_transpose2d(inputs, weights, padding=1)
```

* * *

```py
torch.nn.functional.conv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) → Tensor
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/modules/conv.html#ConvTranspose3d)
在由几个输入平面组成的输入图像上应用3D转置卷积，有时也被称为去卷积。
有关详细信息和输出形状，参考[ConvTranspose3d](https://pytorch.org/docs/master/nn.html#torch.nn.ConvTranspose3d)。

### 参数：

```py
input：输入的张量形状(minibatch x in_channels x iT x iH x iW)
weight – 过滤器的形状 (in_channels x out_channels/groups x kT x kH x kW) 
bias – 可选偏置的形状(out_channels)默认值：None
stride – 卷积内核的步长，也可以是一个数字或者元组(sT,sH，sW），默认为1 
padding – 在输入的两端上的隐含零填充。可以是单个数字或者元组(padT,padH，padW）。默认值:0 
output_padding-在两端的进行0填充(0≤padding<stride），可以是个单一数字或者元组(out_padT, out_padH, out_padW)。默认是0。
dilation – 内核元素之间的间距。可以是单个数字或者元组(dT,dH,dW）。默认值:1 
groups – 将输入分成组，in_channels应该被组数整除。默认值：1
```

### 举例：

```py
>>> inputs = torch.randn(20, 16, 50, 10, 20)
>>> weights = torch.randn(16, 33, 3, 3, 3)
>>> F.conv_transpose3d(inputs, weights)
```

## 池化函数

```py
torch.nn.functional.avg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#avg_pool1d)

对由几个输入平面组成的输入进行1D平均池化。
有关详细信息和输出形状，参考[AvgPool1d](https://pytorch.org/docs/master/nn.html#torch.nn.AvgPool1d)

### 参数：

```py
input – 输入的张量 (minibatch x in_channels x iW)
kernel_size – 池化区域的大小，可以是单个数字或者元组 (kw)
stride – 池化操作的步长，可以是单个数字或者元组 (ssw)。默认值等于内核大小
padding – 在输入上隐式的零填充，可以是单个数字或者一个元组 ( padw)，默认: 0
ceil_mode – 当为True时，公式中将使用ceil而不是floor来计算输出形状。默认值：False
count_include_pad – 当为True时，将包括平均计算中的零填充。默认值：True
```

### 举例：

```py
>>> # pool of square window of size=3, stride=2
>>> input = torch.tensor([[[1,2,3,4,5,6,7]]])
>>> F.avg_pool1d(input, kernel_size=3, stride=2)
tensor([[[ 2.,  4.,  6.]]])
```

* * *

```py
torch.nn.functional.avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) → Tensor
```

[source](https://pytorch.org/docs/master/nn.html#torch.nn.functional.avg_pool2d)

通过步长dh x dw步骤在kh x kw区域中应用二维平均池操作。输出特征的数量等于输入平面的数量。

有关详细信息和输出形状，参考[AvgPool2d](https://pytorch.org/docs/master/nn.html#torch.nn.AvgPool2d)

### 参数：

```py
input – 输入的张量 (minibatch x in_channels x iH x iW)
kernel_size – 池化区域的大小，可以是单个数字或者元组 (kh x kw)
stride – 池化操作的步长，可以是单个数字或者元组 (sh x sw)。默认值等于内核大小
padding – 在输入上隐式的零填充，可以是单个数字或者一个元组 (padh x padw)，默认: 0
ceil_mode – 当为True时，公式中将使用ceil而不是floor来计算输出形状。默认值：False
count_include_pad – 当为True时，将包括平均计算中的零填充。默认值：True
```

* * *

```py
torch.nn.functional.avg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) → Tensor
```

[source](https://pytorch.org/docs/master/nn.html#torch.nn.functional.avg_pool3d)

通过步长dt x dh x dw步骤在kt x kh x kw区域中应用3D平均池操作。输出功能的数量等于输入平面数/ dt。
有关详细信息和输出形状，参考[AvgPool3d](https://pytorch.org/docs/master/nn.html#torch.nn.AvgPool3d)

### 参数：

```py
input – 输入的张量 (minibatch x in_channels x iT x iH x iW)
kernel_size – 池化区域的大小，可以是单个数字或者元组 (kT x kh x kw)
stride – 池化操作的步长，可以是单个数字或者元组 (sT x sh x sw)。默认值等于内核大小
padding – 在输入上隐式的零填充，可以是单个数字或者一个元组 (padT x padh x padw)，默认: 0
ceil_mode – 当为True时，公式中将使用ceil而不是floor来计算输出形状。默认值：False
count_include_pad – 当为True时，将包括平均计算中的零填充。默认值：True
```

* * *

```py
torch.nn.functional.max_pool1d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#max_pool1d)

对由几个输入平面组成的输入进行1D最大池化。
有关详细信息和输出形状，参考[MaxPool1d](https://pytorch.org/docs/master/nn.html#torch.nn.MaxPool1d)

* * *

```py
torch.nn.functional.max_pool2d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#max_pool2d)

对由几个输入平面组成的输入进行2D最大池化。
有关详细信息和输出形状，参考[MaxPool2d](https://pytorch.org/docs/master/nn.html#torch.nn.MaxPool2d)

* * *

```py
torch.nn.functional.max_pool3d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#max_pool3d)

对由几个输入平面组成的输入进行3D最大池化。
有关详细信息和输出形状，参考[MaxPool3d](https://pytorch.org/docs/master/nn.html#torch.nn.MaxPool3d)

* * *

```py
torch.nn.functional.max_unpool1d(input, indices, kernel_size, stride=None, padding=0, output_size=None)
```

计算MaxPool1d的部分逆。

有关详细信息和输出形状，参考[MaxUnPool1d](https://pytorch.org/docs/master/nn.html#torch.nn.MaxUnpool1d)

* * *

```py
torch.nn.functional.max_unpool2d(input, indices, kernel_size, stride=None, padding=0, output_size=None)
```

计算MaxPool2d的部分逆。

有关详细信息和输出形状，参考[MaxUnPool2d](https://pytorch.org/docs/master/nn.html#torch.nn.MaxUnpool2d)

* * *

```py
torch.nn.functional.max_unpool3d(input, indices, kernel_size, stride=None, padding=0, output_size=None)
```

计算MaxPool3d的部分逆。

有关详细信息和输出形状，参考[MaxUnPool3d](https://pytorch.org/docs/master/nn.html#torch.nn.MaxUnpool3d)

* * *

```py
torch.nn.functional.lp_pool1d(input, norm_type, kernel_size, stride=None, ceil_mode=False)
```

适用在几个输入平面组成的输入信号的1D power-平均池。

有关详细信息[LPPool1d](https://pytorch.org/docs/master/nn.html#torch.nn.LPPool1d)

* * *

```py
torch.nn.functional.lp_pool2d(input, norm_type, kernel_size, stride=None, ceil_mode=False)
```

适用在几个输入平面组成的输入信号的2D power-平均池。

有关详细信息[LPPool2d](https://pytorch.org/docs/master/nn.html#torch.nn.LPPool2d)

* * *

```py
torch.nn.functional.adaptive_max_pool1d(input, output_size, return_indices=False)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#adaptive_max_pool1d)

对由多个输入平面组成的输入进行1D自适应最大池化。

有关详细信息可见[AdaptiveMaxPool1d](https://pytorch.org/docs/master/nn.html#torch.nn.AdaptiveMaxPool1d)

* * *

```py
torch.nn.functional.adaptive_max_pool2d(input, output_size, return_indices=False)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#adaptive_max_pool2d)

对由多个输入平面组成的输入进行2D自适应最大池化。

有关详细信息可见[AdaptiveMaxPool2d](https://pytorch.org/docs/master/nn.html#torch.nn.AdaptiveMaxPool2d)

* * *

```py
torch.nn.functional.adaptive_max_pool3d(input, output_size, return_indices=False)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#adaptive_max_pool3d)

对由多个输入平面组成的输入进行3D自适应最大池化。

有关详细信息可见[AdaptiveMaxPool3d](https://pytorch.org/docs/master/nn.html#torch.nn.AdaptiveMaxPool3d)

* * *

```py
torch.nn.functional.adaptive_avg_pool1d(input, output_size) → Tensor
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#adaptive_avg_pool1d)

对由多个输入平面组成的输入进行1D自适应平均池化。

有关详细信息可见[AdaptiveAvgPool1d](https://pytorch.org/docs/master/nn.html#torch.nn.AdaptiveAvgPool1d)

* * *

```py
torch.nn.functional.adaptive_avg_pool2d(input, output_size) → Tensor
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#adaptive_avg_pool2d)

对由多个输入平面组成的输入进行2D自适应平均池化。

有关详细信息可见[AdaptiveAvgPool2d](https://pytorch.org/docs/master/nn.html#torch.nn.AdaptiveAvgPool2d)

* * *

```py
torch.nn.functional.adaptive_avg_pool3d(input, output_size) → Tensor
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#adaptive_avg_pool3d)

对由多个输入平面组成的输入进行3D自适应平均池化。

有关详细信息可见[AdaptiveAvgPool3d](https://pytorch.org/docs/master/nn.html#torch.nn.AdaptiveAvgPool3d)

## 非线性激活函数

```py
torch.nn.functional.threshold(input, threshold, value, inplace=False)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#threshold)

对于输入的张量进行筛选

详细请看[Threshold](https://pytorch.org/docs/master/nn.html#torch.nn.Threshold)

* * *

```py
torch.nn.functional.threshold_(input, threshold, value)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#threshold)

和threshold函数一样

详细请看[Threshold](https://pytorch.org/docs/master/nn.html#torch.nn.functional.threshold)

* * *

```py
torch.nn.functional.relu(input, inplace=False) → Tensor
torch.nn.functional.relu_(input) → Tensor
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#relu)

对输入元素应用relu函数

详细请看[Relu](https://pytorch.org/docs/master/nn.html#torch.nn.ReLU)

* * *

```py
torch.nn.functional.hardtanh(input, min_val=-1., max_val=1., inplace=False) → Tensor
torch.nn.functional.hardtanh_(input, min_val=-1., max_val=1.) → Tensor
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#hardtanh)

对输入元素应用HardTanh函数

详细请看[Hardtanh](https://pytorch.org/docs/master/nn.html#torch.nn.Hardtanh)

* * *

```py
torch.nn.functional.relu6(input, inplace=False) → Tensor
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#relu6)

对输入元素应用ReLU6函数 ReLU6(x)=min(max(0,x),6)

详细请看[ReLU6](https://pytorch.org/docs/master/nn.html#torch.nn.ReLU6)

* * *

```py
torch.nn.functional.elu(input, alpha=1.0, inplace=False)
torch.nn.functional.elu_(input, alpha=1.) → Tensor
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#elu)

对输入元素应用ELU函数 ELU(x)=max(0,x)+min(0,α∗(exp(x)−1))

详细请看[ELU](https://pytorch.org/docs/master/nn.html#torch.nn.ELU)

* * *

```py
torch.nn.functional.selu(input, inplace=False) → Tensor[source]
```

详细可见[selu](https://pytorch.org/docs/master/nn.html#torch.nn.SELU)

* * *

```py
torch.nn.functional.leaky_relu(input, negative_slope=0.01, inplace=False) → Tensor
torch.nn.functional.leaky_relu_(input, negative_slope=0.01) → Tensor
```

详细可见[LeakyReLu](https://pytorch.org/docs/master/nn.html#torch.nn.LeakyReLU)

* * *

```py
torch.nn.functional.prelu(input, weight) → Tensor
```

详细可见[PRelu](https://pytorch.org/docs/master/nn.html#torch.nn.PRelu)

* * *

```py
torch.nn.functional.rrelu(input, lower=1./8, upper=1./3, training=False, inplace=False) → Tensor
torch.nn.functional.rrelu_(input, lower=1./8, upper=1./3, training=False) → Tensor
```

详细可见[RRelu](https://pytorch.org/docs/master/nn.html#torch.nn.RRelu)

* * *

```py
torch.nn.functional.glu(input, dim=-1) → Tensor
```

门控制线性单元 H=A×σ(B),输入张量将被按照特定维度分成一半是A，一半是B

可以参看论文[Language Modeling with Gated Convolutional Networks](https://arxiv.org/abs/1612.08083)

### 参数：

```py
input：输入的张量
dim：需要被分割的输入张量的维度
```

* * *

```py
torch.nn.functional.logsigmoid(input) → Tensor
```

具体细节可以看[LogSigmoid](https://pytorch.org/docs/master/nn.html#torch.nn.LogSigmoid)

* * *

```py
torch.nn.functional.hardshrink(input, lambd=0.5) → Tensor
```

具体细节可以看[Hardshrink](https://pytorch.org/docs/master/nn.html#torch.nn.Hardshrink)

* * *

```py
torch.nn.functional.tanhshrink(input, lambd=0.5) → Tensor
```

具体细节可以看[Tanhshrink](https://pytorch.org/docs/master/nn.html#torch.nn.Tanhshrink)

* * *

```py
torch.nn.functional.softsign(input, lambd=0.5) → Tensor
```

具体细节可以看[Softsign](https://pytorch.org/docs/master/nn.html#torch.nn.Softsign)

* * *

```py
torch.nn.functional.softplus(input, beta=1, threshold=20) → Tensor
```

* * *

```py
torch.nn.functional.softmin(input, dim=None, _stacklevel=3)
```

应用softmin函数 请注意Softmin(x）= Softmax(-x）。 请参阅数学公式的softmax定义。

具体细节可以看[Softmin](https://pytorch.org/docs/master/nn.html#torch.nn.Softmin)

### 参数：

```py
input:输入张量
dim：softmin将被计算的维度(因此每个沿着dim的切分将总计为1）。
```

* * *

```py
torch.nn.functional.softmax(input, dim=None, _stacklevel=3
```

Softmax(x_i)=exp(x_i)/∑_jexp(x_j)

它被应用于沿着对应维度的所有切分，并且将对它们进行重新缩放，以使得这些元素位于范围(0,1）中并且总计为1。

具体细节可以看[Softmax](https://pytorch.org/docs/master/nn.html#torch.nn.Softmax)

### 参数：

```py
input:输入张量
dim：softmax被计算的维度。
```

* * *

```py
torch.nn.functional.softshrink(input, lambd=0.5) → Tensor
```

具体细节可以看[Softshrink](https://pytorch.org/docs/master/nn.html#torch.nn.Softshrink)

* * *

```py
torch.nn.functional.log_softmax(input, dim=None, _stacklevel=3)
```

尽管在数学上等同于log(softmax(x）），但单独执行这两个操作会更慢，并且数值不稳定。 该函数使用替代公式来正确计算输出和梯度。

具体细节可以看[LogSoftmax](https://pytorch.org/docs/master/nn.html#torch.nn.LogSoftmax)

* * *

```py
torch.nn.functional.tanh(input) → Tensor
```

具体细节可以看[Tanh](https://pytorch.org/docs/master/nn.html#torch.nn.Tanh)

* * *

```py
torch.nn.functional.sigmoid(input) → Tensor
```

sigmoid(x) = 1/1+exp(-x)

具体细节可以看[Sigmoid](https://pytorch.org/docs/master/nn.html#torch.nn.Sigmoid)

## 归一化函数

* * *

```py
torch.nn.functional.batch_norm(input, running_mean, running_var, weight=None, bias=None, training=False, momentum=0.1, eps=1e-05)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#batch_norm)

具体细节可以看[BatchNorm1d](https://pytorch.org/docs/master/nn.html#torch.nn.BatchNorm1d) [BatchNorm2d](https://pytorch.org/docs/master/nn.html#torch.nn.BatchNorm2d) [BatchNorm3d](https://pytorch.org/docs/master/nn.html#torch.nn.BatchNorm3d)

* * *

```py
torch.nn.functional.instance_norm(input, running_mean=None, running_var=None, weight=None, bias=None, use_input_stats=True, momentum=0.1, eps=1e-05)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#instance_norm)

具体细节可以看[InstanceNorm1d](https://pytorch.org/docs/master/nn.html#torch.nn.InstanceNorm1d) [InstanceNorm2d](https://pytorch.org/docs/master/nn.html#torch.nn.InstanceNorm2d) [InstanceNorm3d](https://pytorch.org/docs/master/nn.html#torch.nn.InstanceNorm3d)

* * *

```py
torch.nn.functional.layer_norm(input, normalized_shape, weight=None, bias=None, eps=1e-05)
```

具体细节可以看[LayerNorm](https://pytorch.org/docs/master/nn.html#torch.nn.LayerNorm)

* * *

```py
torch.nn.functional.local_response_norm(input, size, alpha=0.0001, beta=0.75, k=1)
```

对由多个输入平面组成的输入应用本地响应规范化，其中通道占据第二维。 通道应用归一化。

具体细节可以看[LocalResponseNorm](https://pytorch.org/docs/master/nn.html#torch.nn.LocalResponseNorm)

* * *

```py
torch.nn.functional.normalize(input, p=2, dim=1, eps=1e-12)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#linear)

对指定维度的输入执行$$L_p$$标准化。

$$v = \frac{v}{\max(\lVert v \rVert_p, \epsilon)}$$

对于输入的维度dim的每个子扩展v。 每个子扩张被平化成一个向量，即‖v‖p不是一个矩阵范数。

使用默认参数在第二维上用欧几里得范数进行归一化。

### 参数：

```py
input - 输入张量的形状
p(float） - 规范公式中的指数值。默认值：2
dim(int） - 要缩小的维度。默认值：1
eps(float） - 小值以避免除以零。默认值：1e-12
```

* * *

## 线性函数

```py
torch.nn.functional.linear(input, weight, bias=None)
```

对于输入数据进行线性变化:$$y = xA^T+b$$.

### 形状：

```py
Input: (N,∗,in_features)(N,∗,in_features)这里的*表示为任意数量的附加维度
Weight: (out_features,in_features)(out_features,in_features)
Bias: (out_features)(out_features)
Output: (N,∗,out_features)
```

* * *

## Dropout 函数

```py
torch.nn.functional.dropout(input, p=0.5, training=False, inplace=False)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#dropout)

* * *

```py
torch.nn.functional.alpha_dropout(input, p=0.5, training=False)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#alpha_dropout)

详细可见[alpha_dropout](https://pytorch.org/docs/master/nn.html#torch.nn.AlphaDropout)

### 参数：

```py
p(float,optional)-丢弃的可能性，默认是0.5
training(bool,optinal)-在训练模型和验证模型之间的切换，默认是false
```

* * *

```py
torch.nn.functional.dropout2d(input, p=0.5, training=False, inplace=False)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#dropout2d)

* * *

```py
torch.nn.functional.dropout3d(input, p=0.5, training=False, inplace=False)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#dropout3d)

* * *

## 距离函数

```py
torch.nn.functional.pairwise_distance(x1, x2, p=2, eps=1e-06, keepdim=False)
```

详细可见 [PairwiseDistance](https://pytorch.org/docs/master/nn.html#torch.nn.PairwiseDistance)

* * *

```py
torch.nn.functional.cosine_similarity(x1, x2, dim=1, eps=1e-08)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#cosine_similarity)

计算向量v1、v2之间的距离 $$similarity = \frac{x_1 x x_2}{\max(\lVert v_1 \rVert_2 x \max(\lVert v_2 \rVert_2,\epsilon)}$$

### 参数：

```py
x1 (Variable) – 首先输入参数.
x2 (Variable) – 第二个输入参数 (of size matching x1).
dim (int, optional) – 向量维数. 默认为: 1
eps (float, optional) – 小值避免被零分割。默认为: 1e-8 模型：
```

### 形状：

```py
input: (∗1,D,∗2)(∗1,D,∗2) D是位置维度
output: (∗1,∗2)(∗1,∗2) 1是位置维度
```

### 举例：

```py
>>> input1 = autograd.Variable(torch.randn(100, 128))
>>> input2 = autograd.Variable(torch.randn(100, 128))
>>> output = F.cosine_similarity(input1, input2)
>>> print(output)
```

## 损失函数

```py
torch.nn.functional.binary_cross_entropy(input, target, weight=None, size_average=True, reduce=True)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#binary_cross_entropy)

测量目标和输出之间的二进制交叉熵的函数。

详细可见[BCELoss](https://pytorch.org/docs/master/nn.html#torch.nn.BCELoss)

### 参数：

```py
input:任意维度
target：与输入维度相同
weight(张量，可选）：如果提供的权重矩阵能匹配输入张量形状，则手动调整重量
size_average(布尔值，可选)：默认情况下，对每个小批次的损失进行平均观察。 但是，如果field size_average设置为False，则每个小批次的损失将相加。 默认值：True
reduce(布尔值，可选）:默认情况下，根据size_average的不同，对每个小批次的损失进行平均或累计。 当reduce为False时，将返回每个输入/目标元素的损失，而忽略size_average。 默认值：True
```

### 举例：

```py
>>> input = torch.randn((3, 2), requires_grad=True)
>>> target = torch.rand((3, 2), requires_grad=False)
>>> loss = F.binary_cross_entropy(F.sigmoid(input), target)
>>> loss.backward()
```

* * *

```py
torch.nn.functional.poisson_nll_loss(input, target, log_input=True, full=False, size_average=True, eps=1e-08, reduce=True)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#poisson_nll_loss)

泊松负对数似然损失 详细可见[PoissonNLLLoss](https://pytorch.org/docs/master/nn.html#torch.nn.PoissonNLLLoss)

### 参数：

```py
input：按照泊松分布的期望
target：随机样例target~Poisson(input）
log_input:如果为真，则损失计算为exp(inout）-target * input，如果为False，则input-target *log⁡(input + eps）。 默认值：True
full:是否计算完全损失。即添加斯特林近似项。 默认值：False target * log(target）-target + 0.5 * log(2 *π* target)
size_average:默认情况下，对每个小批次的损失进行平均观察。 但是，如果field size_average设置为False，则每个小批次的损失将相加。 默认值：True
eps(float，可选） - 当log_input`=“False”时避免评估log(0）log⁡(0）的较小值。 默认：1e-8
reduce(布尔值，可选）:默认情况下，根据每个小批次的观测结果对损失进行平均，或根据size_average进行汇总。 如果reduce为False，则返回每批损失，并忽略size_average。 默认值：True
```

* * *

```py
torch.nn.functional.cosine_embedding_loss(input1, input2, target, margin=0, size_average=True, reduce=True) → Tensor
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#cosine_embedding_loss)

详细可见[CosineEmbeddingLoss](https://pytorch.org/docs/master/nn.html#torch.nn.CosineEmbeddingLoss)

* * *

```py
torch.nn.functional.cross_entropy(input, target, weight=None, size_average=True, ignore_index=-100, reduce=True)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#cross_entropy)

该标准将log_softmax和nll_loss结合在一个函数中。

[CrossEntropyLoss](https://pytorch.org/docs/master/nn.html#torch.nn.CrossEntropyLoss)

### 参数：

```py
input(张量）- (N,C) 其中，C 是类别的个数
target(张量）- (N) 其大小是 0 <= targets[i] <= C-1 1\. weight (Variable, optional) – (N) 其大小是 0 <= targets[i] <= C-1
weight (张量, optional) – 为每个类别提供的手动权重。如果给出，必须是大小为C的张量
size_average (bool, optional) – 默认情况下，是mini-batchloss的平均值；如果size_average=False，则是mini-batchloss的总和。
ignore_index(int，可选） - 指定被忽略且不对输入渐变有贡献的目标值。当size_average为True时，对非忽略目标的损失是平均的。默认值：-100
reduce(布尔值，可选）-默认情况下，根据每个小批次的观测结果对损失进行平均，或根据size_average进行汇总。 如果reduce为False，则返回每批损失，并忽略size_average。 默认值：True
```

### 举例：

```py
>>> input = torch.randn(3, 5, requires_grad=True)
>>> target = torch.randint(5, (3,), dtype=torch.int64)
>>> loss = F.cross_entropy(input, target)
>>> loss.backward()
```

* * *

```py
torch.nn.functional.hinge_embedding_loss(input, target, margin=1.0, size_average=True, reduce=True) → Tensor
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#hinge_embedding_loss)

详细可见[HingeEmbeddingLoss](https://pytorch.org/docs/master/nn.html#torch.nn.HingeEmbeddingLoss)

* * *

```py
torch.nn.functional.kl_div(input, target, size_average=True) → Tensor
```

[source](https://pytorch.org/docs/master/nn.html#torch.nn.functional.kl_div)

The [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback-Leibler_divergence) Loss

详细可见[KLDivLoss](https://pytorch.org/docs/master/nn.html#torch.nn.KLDivLoss)

### 参数：

```py
input – 变量的任意形状
target - 与输入相同形状的变量
size_average – 如果是真的，输出就除以输入张量中的元素个数
reduce(布尔值，可选）-默认情况下，根据每个小批次的观测结果对损失进行平均，或根据size_average进行汇总。 如果reduce为False，则返回每批损失，并忽略size_average。 默认值：True
```

* * *

```py
torch.nn.functional.l1_loss(input, target, size_average=True, reduce=True) → Tensor
```

详细可见[L1Loss](https://pytorch.org/docs/master/nn.html#torch.nn.L1Loss)

* * *

```py
torch.nn.functional.mse_loss(input, target, size_average=True, reduce=True) → Tensor
```

详细可见[MSELoss](https://pytorch.org/docs/master/nn.html#torch.nn.MSELoss)

* * *

```py
torch.nn.functional.margin_ranking_loss(input, target, size_average=True, reduce=True) → Tensor
```

详细可见[MarginRankingLoss](https://pytorch.org/docs/master/nn.html#torch.nn.MarginRankingLoss)

* * *

```py
torch.nn.functional.multilabel_soft_margin_loss(input, target, size_average=True, reduce=True) → Tensor
```

详细可见[MultiLabelSoftMarginLoss](https://pytorch.org/docs/master/nn.html#torch.nn.MultiLabelSoftMarginLoss)

* * *

```py
torch.nn.functional.multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=True, reduce=True) → Tensor
```

详细可见[MultiMarginLoss](https://pytorch.org/docs/master/nn.html#torch.nn.MultiMarginLoss)

* * *

```py
torch.nn.functional.nll_loss(input, target, weight=None, size_average=True, ignore_index=-100, reduce=True)
```

详细可见[NLLLoss](https://pytorch.org/docs/master/nn.html#torch.nn.NLLLoss)

### 参数：

```py
input - \((N，C）\）其中C =类的数量或 (2D，Loss )的情况下的(N，C，H，W） target - \((N）\），其中每个值为0 <= targets [i] <= C-1
weight(可变，可选） - 给每个类别的手动重新调整重量。如果给定，必须变量大小是C
size_average(bool，可选） - 默认情况下，损失是对每个小型服务器的观察值进行平均。如果size_average为False，则对于每个minibatch都会将损失相加。默认值：True
ignore_index(int，可选） - 指定被忽略且不对输入渐变有贡献的目标值。当size_average为True时，对非忽略目标的损失是平均的。默认值：-100
```

### 举例：

```py
>>> # input is of size N x C = 3 x 5
>>> input = torch.randn(3, 5, requires_grad=True)
>>> # each element in target has to have 0 <= value < C
>>> target = torch.tensor([1, 0, 4])
>>> output = F.nll_loss(F.log_softmax(input), target)
>>> output.backward()
```

* * *

```py
torch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, size_average=True, reduce=True)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#binary_cross_entropy_with_logits)

测量目标和输出逻辑之间二进制十进制熵的函数

详细可见[BCEWithLogitsLoss](https://pytorch.org/docs/master/nn.html#torch.nn.BCEWithLogitsLoss)

### 参数：

```py
input - 任意形状的变量
target - 与输入形状相同的变量
weight(可变，可选） - 手动重量重量，如果提供重量以匹配输入张量形状
size_average(bool，可选） - 默认情况下，损失是对每个小型服务器的观察值进行平均。然而，如果字段sizeAverage设置为False，则相应的损失代替每个minibatch的求和。默认值：True
reduce(布尔值，可选）-默认情况下，根据每个小批次的观测结果对损失进行平均，或根据size_average进行汇总。 如果reduce为False，则返回每批损失，并忽略size_average。 默认值：True
```

### 举例：

```py
>>> input = torch.randn(3, requires_grad=True)
>>> target = torch.empty(3).random_(2)
>>> loss = F.binary_cross_entropy_with_logits(input, target)
>>> loss.backward()
```

* * *

```py
torch.nn.functional.smooth_l1_loss(input, target, size_average=True, reduce=True) → Tensor
```

详细可见[SmoothL1Loss](https://pytorch.org/docs/master/nn.html#torch.nn.SmoothL1Loss)

* * *

```py
torch.nn.functional.soft_margin_loss(input, target, size_average=True, reduce=True) → Tensor
```

详细可见[Soft_margin_loss](https://pytorch.org/docs/master/nn.html#torch.nn.Soft_margin_loss)

* * *

```py
torch.nn.functional.triplet_margin_loss(anchor, positive, negative, margin=1.0, p=2, eps=1e-06, swap=False, size_average=True, reduce=True)
```

详细可见[TripletMarginLoss](https://pytorch.org/docs/master/nn.html#torch.nn.TripletMarginLoss)

## 视觉函数

```py
torch.nn.functional.pixel_shuffle(input, upscale_factor)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#pixel_shuffle)

将形状为[_, C_r^2, H, W]的张量重新排列成形状为[C, H_r, W_r]的张量.

详细请看[PixelShuffle](https://pytorch.org/docs/master/nn.html#torch.nn.PixelShuffle)

### 参数：

```py
input (Variable) – 输入
upscale_factor (int) – 增加空间分辨率的因子.
```

### 举例：

```py
>>> ps = nn.PixelShuffle(3)
>>> input = autograd.Variable(torch.Tensor(1, 9, 4, 4))
>>> output = ps(input)
>>> print(output.size())
torch.Size([1, 1, 12, 12])
```

* * *

```py
torch.nn.functional.pad(input, pad, mode='constant', value=0)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#pad)

填充张量.

可以参考[torch.nn.ConstantPad2d](https://pytorch.org/docs/master/nn.html#torch.nn.ConstantPad2d), [torch.nn.ReflectionPad2d](https://pytorch.org/docs/master/nn.html#torch.nn.ReflectionPad2d), 和 [torch.nn.ReplicationPad2d](https://pytorch.org/docs/master/nn.html#torch.nn.ReplicationPad2d)对于每个填充模式如何工作的具体例子。

### 参数：

```py
input (Variable) – 4D 或 5D tensor
pad (tuple) – 4元素 或 6-元素 tuple
mode – ‘constant’, ‘reflect’ or ‘replicate’
value – 用于constant padding 的值.
```

### 举例：

```py
>>> t4d = torch.empty(3, 3, 4, 2)
>>> p1d = (1, 1) # pad last dim by 1 on each side
>>> out = F.pad(t4d, p1d, "constant", 0)  # effectively zero padding
>>> print(out.data.size())
torch.Size([3, 3, 4, 4])
>>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)
>>> out = F.pad(t4d, p2d, "constant", 0)
>>> print(out.data.size())
torch.Size([3, 3, 8, 4])
>>> t4d = torch.empty(3, 3, 4, 2)
>>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)
>>> out = F.pad(t4d, p3d, "constant", 0)
>>> print(out.data.size())
torch.Size([3, 9, 7, 3])
```

* * *

```py
torch.nn.functional.upsample(input, size=None, scale_factor=None, mode='nearest', align_corners=None)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#upsample)

Upsamples输入内容要么就是给定的size或者scale_factor 用于采样的算法是由模型决定的 目前支持的是空间和容量的采样，即期望输入的形状是4-d或5-d。 输入尺寸被解释为:迷你批x通道x深度x高度x宽度 用于upsampling的模型是:线性的(仅3D)，双线性的(仅4D)，三线性(仅5D)

### 参数：

```py
input (Variable) – 输入内容
size (int or Tuple[int, int] or Tuple[int, int, int]) – 输出空间的大小。
scale_factor (int) – 乘数的空间大小。必须是一个整数。
mode (string) – 用于向上采样的算法: ‘nearest’ | ‘bilinear’ | ‘trilinear’
align_corners(bool，可选） - 如果为True，则输入和输出张量的角点像素对齐，从而保留这些像素的值。 这只在模式为线性，双线性或三线性时才有效。 默认值：False
```

### 警告：

```py
使用align_corners = True时，线性插值模式(线性，双线性和三线性）不会按比例对齐输出和输入像素，因此输出值可能取决于输入大小。
这是这些模式到版本0.3.1的默认行为。 此后，默认行为是align_corners = False。 有关这将如何影响输出的具体示例，请参见Upsample。
```

* * *

```py
torch.nn.functional.upsample_nearest(input, size=None, scale_factor=None)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#upsample_nearest) 使用最接近的邻居的像素值来对输入进行采样。

### 警告：

```py
此功能已弃用，以支持torch.nn.functional.upsample(）。 这相当于nn.functional.upsample(...，mode ='nearest'）。
```

目前支持空间和体积上采样(即预期的输入是4或5维）。

### 参数：

```py
input (Variable) – 输入内容
size (int or Tuple[int, int]) – 输出空间的大小。
scale_factor (int or Tuple[int, int]) – 乘数的空间大小
```

* * *

```py
torch.nn.functional.upsample_bilinear(input, size=None, scale_factor=None)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#upsample_bilinear) 使用双线性向上采样来扩展输入

### 警告：

```py
这个函数是被弃用的。使用nn.functional。upsample相反 预期的输入是空间的(4维)。
使用upsampletri线性来进行体积(5维)输入。
```

### 参数：

```py
input (Variable) – 输入内容
size (int or Tuple[int, int]) – 输出空间的大小。
scale_factor (int or Tuple[int, int]) – 乘数的空间大小
```

* * *

```py
torch.nn.functional.grid_sample(input, grid, mode='bilinear', padding_mode='zeros')
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#grid_sample)

给定输入和流场网格，使用网格中的输入像素位置计算输出。

使用双线性插值来对输入像素进行采样。 目前，仅支持空间(4维）和体积(5维）输入。

对于每个输出位置，网格具有用于计算输出的x，y输入像素位置。 在5D输入的情况下，网格具有x，y，z像素位置。

grid的值在[-1，1]的范围内。 这是因为像素位置是由输入高度和宽度标准化的。

例如，值：x：-1，y：-1是输入的左上像素，值：x：1，y：1是输入的右下像素。

如果grid的值超出[-1，1]的范围，则按padding_mode的定义处理这些位置。 选项为零或边框，定义这些位置以使用0或图像边界值作为对双线性插值的贡献。

### 参数：

```py
input(Tensor） - 输入批次(N x C x IH x IW）或(N x C x ID x IH x IW）
grid(Tensor） - 尺寸(N x OH x OW x 2）或(N x OD x OH x OW x 3）的流场，
padding_mode(str） - 用于外部网格值“零”|的填充模式 '边境'。 默认值：'零'
```

### 返回：

```py
output(tensor）
```

* * *

```py
torch.nn.functional.affine_grid(theta, size)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/functional.html#affine_grid)

生成一个2d流场，给定一批仿射矩阵theta通常与grid_sample(）一起使用来实现Spatial Transformer Networks。

### 参数：

```py
theta(Tensor） - 输入一批仿射矩阵(N×2×3N×2×3）
size(torch.Size） - 目标输出图像尺寸(N×C×H×WN×C×H×W）例如：torch.Size((32,3,24,24））
```

### 返回：

```py
output(tensor）:输出张量大小 (N×H×W×2N×H×W×2)
```

## 并行函数(多GPU，分布式）

```py
torch.nn.parallel.data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None)
```

[source](https://pytorch.org/docs/master/_modules/torch/nn/parallel/data_parallel.html#data_parallel)

在device_ids中给出的GPU上并行评估模块(输入）。 这是DataParallel模块的功能版本。

### 参数：

```py
module - 并行评估的模块
input - 输入到模块
device_ids - 要在其上复制模块的GPU ID
output_device - 输出的GPU位置使用-1指示CPU。 (默认：device_ids [0]）
```

### 返回：

```py
包含位于output_device上的模块(输入）结果的张量
```

### 译者署名

| 用户名 | 头像 | 职能 | 签名 |
| --- | --- | --- | --- |
| travel | ![](https://www.ptorch.com/uploads/2018051622472492229.jpg) | 翻译 | 人生总要追求点什么 |

