# torch.utils.bottleneck

> 原文： [https://pytorch.org/docs/stable/bottleneck.html](https://pytorch.org/docs/stable/bottleneck.html)

&lt;cite&gt;torch.utils.bottleneck&lt;/cite&gt; 是可以用作调试程序瓶颈的第一步的工具。 它使用 Python 分析器和 PyTorch 的 autograd 分析器总结了脚本的运行。

使用以下命令在命令行上运行

```
python -m torch.utils.bottleneck /path/to/source/script.py [args]

```

其中[args]是 &lt;cite&gt;script.py&lt;/cite&gt; 的任意数量的参数，或者运行`python -m torch.utils.bottleneck -h`以获取更多使用说明。

警告

因为您的脚本将被分析，所以请确保它在有限的时间内退出。

Warning

由于 CUDA 内核的异步特性，当针对 CUDA 代码运行时，cProfile 输出和 CPU 模式自动分级探查器可能无法显示正确的计时：报告的 CPU 时间报告了用于启动内核的时间量，但不包括时间 除非操作进行了同步，否则内核将花费在 GPU 上执行。 在常规的 CPU 模式分析器下，进行同步的操作似乎非常昂贵。 在这些时间不正确的情况下，CUDA 模式自动毕业分析器可能会有所帮助。

注意

要确定要查看哪个(仅 CPU 模式或 CUDA 模式）autograd Profiler 输出，您应该首先检查脚本是否受 CPU 限制(“ CPU 总时间远大于 CUDA 总时间”）。 如果它是 CPU 绑定的，则查看 CPU 模式的 autograd profiler 的结果将有所帮助。 另一方面，如果您的脚本将其大部分时间都花在 GPU 上执行，则有必要在 CUDA 模式 autograd profiler 的输出中开始寻找负责任的 CUDA 运算符。

当然，实际情况要复杂得多，根据您要评估的模型部分，您的脚本可能不会处于这两种极端情况之一。 如果分析器输出无济于事，则可以尝试使用`nvprof`查看 [`torch.autograd.profiler.emit_nvtx()`](autograd.html#torch.autograd.profiler.emit_nvtx "torch.autograd.profiler.emit_nvtx") 的结果。 但是，请考虑到 NVTX 开销非常高，并且通常会出现严重偏差的时间表。

Warning

如果您正在分析 CUDA 代码，则运行`bottleneck`的第一个分析器(cProfile）将在其时间报告中包括 CUDA 启动时间(CUDA 缓冲区分配成本）。 瓶颈是否导致代码比 CUDA 启动时间慢得多，这无关紧要。

有关探查器的更复杂用法(例如在多 GPU 情况下），请参阅 [https://docs.python.org/3/library/profile.html](https://docs.python.org/3/library/profile.html) 或 [`torch.autograd.profiler.profile()`](autograd.html#torch.autograd.profiler.profile "torch.autograd.profiler.profile") 了解更多信息。